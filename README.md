# ğŸ§  Hybrid Hopfield-SSM Language Model  

![Params](https://img.shields.io/badge/Params-107M-blue)
![Dataset](https://img.shields.io/badge/Dataset-WikiText--103-green)
![Framework](https://img.shields.io/badge/Framework-JAX%2FFlax-orange)
![Perplexity](https://img.shields.io/badge/Val_PPL-30.3-yellow)

**An experimental Hybrid Language Model Architecture Combining State Space Models, Hopfield Networks, and Attention**  
*Built in JAX / Flax for TPU-scale research (and itâ€™s cheaper by hours on Colab)*  
*(And yes, I also use Emojis ğŸ¥¹)*

---

## ğŸ§© Overview

This is an end-to-end exploration of a custom language-model architecture inspired by biological computation.  
It evolves from early neuro-symbolic ideas into a performant hybrid system that fuses:

- **State Space Models (SSM)** for efficient long-range sequence processing  
- **Modern Hopfield Networks** for associative memory and robust pattern completion  
- **Causal Self-Attention** for content-based reasoning  

These components are dynamically routed through a learned **Gated Mixer**, guided by a lightweight hierarchical controller (**HRM-lite**) that performs segmented reasoning.  
Training and evaluation were performed from scratch on **WikiText-103** (and *Tiny Shakes*, see early README) at both **word-level** (PPL â‰ˆ 30 @ 25 k steps) and **subword (BPE)** levels (PPL â‰ˆ 35 @ 25 k steps) using **JAX / Flax** on TPUs.

---

## âœ¨ Key Features

| Capability | Description |
|-------------|-------------|
| **Hybrid Architecture** | Unified block combining SSM, MoR-Hopfield, and Self-Attention. |
| **High Performance** | Validation PPL â‰ˆ 30.3 (word-level WikiText-103, ~107 M params). |
| **Efficient Implementation** | > 60 K tokens / s on TPU v3. |
| **Segmented Reasoning (HRM-lite)** | Two-segment forward loop with a stop-gradient controller bias. |
| **Emergent Component Roles** | Probes show Hopfield is position-sensitive, Attention content-sensitive. |
| **Stable Training** | Rectified Flow regularization, label smoothing, and z-loss prevent collapse. |

---

## Final Architectural Deep Dive (and mermaid graph)

The final model (~107 M params) is an 8-layer decoder-only LM built around a `HybridBlock` that merges three computation paths:

```mermaid
graph TD
    A[Input] --> B[Token Embeddings]
    B --> C[Positional Embedding]
    C --> H[hâ‚€]

    subgraph Segmented_Reasoning
        H --> S1[Segment 1]
        S1 -->|stop_gradient| CTRL[Controller]
        CTRL -->|bias| S2[Segment 2]
    end

    S2 --> LM[LM Head]
    LM --> O[Logits]

    %% Styling
    style A fill:#ffe6ff,stroke:#333,stroke-width:1.5px
    style O fill:#e6f0ff,stroke:#333,stroke-width:1.5px
    style Segmented_Reasoning fill:#fdfdfd,stroke:#999,stroke-width:1px
```

### 1ï¸âƒ£ Modern SSM (`ModernSSM`)
- Depthwise causal conv prefilter â†’ selective scan (A diag param via log stabilization)  
- Input-dependent gates (`g_in`, `g_forget`) control temporal propagation  
**Role:** Capture long-range dependencies efficiently  

### 2ï¸âƒ£ Mixture-of-Recursions Hopfield (`MoRHopfield`)
- Performs *K* recursive refinements (K scheduled per layer, e.g. [3, 2, 2, â€¦ 1])  
  *(L0 layer proved most impactful after several ablations)*  
- Learns to gate across recursion depths for shallow vs deep recall  
**Role:** Robust pattern completion and associative memory  

### 3ï¸âƒ£ Causal Self-Attention (`SelfAttention`)
- Standard multi-head attention with causal mask  
**Role:** Content-based pattern mixing within the context window  

### 4ï¸âƒ£ Gated Mixer (`GatedMixer`)
- Small feed-forward net generating softmax gates over SSM, Hopfield, and Attention outputs  
**Role:** Learned information routing per context  

### 5ï¸âƒ£ Segmented Reasoner & Controller (`SegmentReasoner`, `HController`)
- Two-segment forward loop; Segment 1 produces a hidden state fed to `HController`, which outputs a bias vector to influence Segment 2â€™s mixing weights  
- A `stop_gradient` approximates one-step credit assignment  
**Role:** Iterative refinement without explicit RL  

---

## ğŸ“Š Performance and Experimental Results

### ğŸ Main Results (WikiText-103 word-level)

| Metric | Value |
|:--|:--|
| Best Validation Cross-Entropy | 3.4424 |
| Best Validation Perplexity | 30.3 |
| Training Steps | 25 000 |
| Parameters | â‰ˆ 107 M |
| Throughput (TPU v3 / Colab) | â‰ˆ 60 K tok/s |

A BPE-tokenized variant (~97 M params) reached PPL â‰ˆ 36.8.

---

## ğŸ“ˆ Training Dynamics

Early byte-level experiments confirmed stable learning curves with steady CE reduction, validating the architecture before scaling to word level.

*(Figure 1 placeholder â€” Training and Validation CE curves)*

---

## ğŸ”¬ Ablation Studies â€” Importance of Hybridization

| Configuration | Validation CE | Category |
|:--|:--:|:--|
| Full Hybrid Model | 3.60 | Baseline |
| Only SSM | 9.81 | Single Path |
| Only Hopfield | 7.72 | Single Path |
| Only Attention | 9.15 | Single Path |
| SSM + Hopfield | 5.43 | Two Paths |
| SSM + Attention | 9.51 | Two Paths |
| Hopfield + Attention | 7.40 | Two Paths |

**Conclusion:** All three components are necessary; removing any causes a sharp performance drop.

---

## ğŸ§© Probe Analysis â€” Component Roles

| Intervention | Validation CE | Observation |
|:--|:--:|:--|
| Baseline (no shuffle) | 3.59 | â€” |
| Shuffle â†’ Attention | 4.35 | Moderate impact |
| Shuffle â†’ SSM | 6.00 | Strong impact |
| Shuffle â†’ Hopfield | 8.01 | Severe degradation |

**Findings**  
- Hopfield modules encode position-dependent patterns  
- Attention focuses on content irrespective of position  
- SSM balances both, bridging sequential and content signals  

*(Table 2 â€” Probe Results)*
| Test | Val CE | Category |
|:--|--:|:--|
| baseline | 3.6043 | baseline |
| seg_1 | 3.8902 | segments |
| seg_2 | 3.6136 | segments |
| seg_3 | 3.6043 | segments |
| ctrl_normal | 3.6043 | controller |
| ctrl_shuffle | 3.7270 | controller |
| ctrl_random | 4.1094 | controller |
| no_ctrl | 3.8902 | controller |
| only_ssm | 9.8124 | single_path |
| only_hop | 7.7175 | single_path |
| only_attn | 9.1533 | single_path |
| ssm+hop | 5.4290 | two_path |
| ssm+attn | 9.5113 | two_path |
| hop+attn | 7.4022 | two_path |

---

## ğŸ”„ Project Evolution

| Phase | Focus | Milestone |
|:--|:--|:--|
| 1ï¸âƒ£ PyTorch Prototype | HippocampalMemoryInterface & Delayed Copy Task | Proof of concept |
| 2ï¸âƒ£ Component Comparison | Recurrent vs Hopfield CA3 | Hopfield proved superior |
| 3ï¸âƒ£ Hybridization | Combine SSM + Hopfield + Attention on TinyShakespeare | Stable training |
| 4ï¸âƒ£ JAX / Flax Migration | TPU performance + byte-level WikiText | Improved efficiency |
| 5ï¸âƒ£ Scaling | Word-level WikiText-103 (~107 M params) | Final architecture + MoR / HRM-lite |
| 6ï¸âƒ£ Advanced Concepts | Rectified Flow regularization | Stable long-runs and probes |

---

## âš™ï¸ Usage

### Setup
```bash
pip install jax flax optax datasets tokenizers

## Training

**Training**

# From training_bpe.py
if __name__ == "__main__":
    train_bpe_level()


## Sampling

import jax, pickle
from tokenizers import Tokenizer
import training_bpe as mdl

with open("path/to/best.ckpt", "rb") as f:
    ckpt = pickle.load(f)
tokenizer = Tokenizer.from_file(ckpt["tokenizer_path"])

prompt = "The history of artificial intelligence began in"
generated = mdl.sample_text(
  state=ckpt['ema_params'],
  base_cfg=ckpt['base_cfg'],
  tokenizer=tokenizer,
  prompt=prompt,
  max_new_tokens=100,
)
print(generated)



## Code Structure

ProjectHippocampus/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hybrid_lm.py          # HybridLM wrapper
â”‚   â”œâ”€â”€ hybrid_block.py       # Core HybridBlock
â”‚   â”œâ”€â”€ modern_ssm.py         # SSM implementation
â”‚   â”œâ”€â”€ mor_hopfield.py       # Mixture-of-Recursions Hopfield
â”‚   â”œâ”€â”€ gated_mixer.py        # Learned gating
â”‚   â””â”€â”€ controller.py         # HRM-lite controller
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ training_bpe.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ rectified_flow.py
â”œâ”€â”€ probes/
â”‚   â”œâ”€â”€ shuffle_probes.py
â”‚   â””â”€â”€ results.csv
â”œâ”€â”€ checkpoints/
â””â”€â”€ README.md



## Citations

@misc{adam2025hippocampus,
  title  = {Hybrid what? A Hybrid Language Model Architecture},
  author = {Julian Adam},
  year   = {2025},
  note   = {Hybrid SSM + Hopfield + Attention; HRM-lite controller; WikiText-103 word-level PPL â‰ˆ 30.3. (BPE 35)}
}


## License

MIT License â€” see MIT LICENSE.
---


## ğŸ§¾ Changelog
**v1.0 â€“ Oct 2025**
- Initial public release (WikiText-103 word-level PPL â‰ˆ 30.3)
- Added probes and ablation tables
- Added segmented reasoning diagram
- Still need to add the code, its still one huge monolith across a few jupyter cells and .py's lol

