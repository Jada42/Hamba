
â¸»

Hopfield-SSM-Project 

A Hybrid Language Model Architecture Combining State Space Models, Hopfield Networks, and Attention
Built in JAX / Flax for TPU-scale research (and its cheaper by hours on colab)
(And yes, I also use Emojis ğŸ¥¹)

â¸»

ğŸ§© Overview

This is an end-to-end exploration of a custom language-model architecture inspired by biological computation.
It evolves from early neuro-symbolic ideas into a performant hybrid system that fuses:
	â€¢	State Space Models (SSM) for efficient long-range sequence processing,
	â€¢	Modern Hopfield Networks for associative memory and robust pattern completion, and
	â€¢	Causal Self-Attention for content-based reasoning.

These components are dynamically routed through a learned Gated Mixer, guided by a lightweight hierarchical controller (HRM-lite) that performs segmented reasoning.
Training and evaluation were performed from scratch on WikiText-103 (and tinyshakes, see early Readme) at both word-(PPL 30 at 25k steps) and subword-(BPE-PPL 35 at 25k steps) levels using JAX/Flax on TPUs.

â¸»

âœ¨ Key Features

Capability	Description
Hybrid Architecture	Unified block combining SSM, MoR-Hopfield, and Self-Attention.
High Performance	Validation PPL â‰ˆ 30.3 (word-level WikiText-103, ~107 M params).
Efficient Implementation	> 60 K tokens / s on TPU v3.
Segmented Reasoning (HRM-lite)	Two-segment forward loop with a stop-gradient controller bias.
Emergent Component Roles	Probes show Hopfield is position-sensitive, Attention content-sensitive.
Stable Training	Rectified Flow regularization, label smoothing, and z-loss prevent collapse.


â¸»

 Final Architecture Deep Dive

The final model (~107 M params) is an 8-layer decoder-only LM built around a HybridBlock that merges three computation paths:

Input â†’ Positional Embedding â†’ hâ‚€
   â”œâ”€â–º Segment 1 â”€â”
   â”‚              â”‚ stop_gradient
   â”‚           Controller â†’ bias
   â””â”€â–º Segment 2 â”€â”˜ â†’ LM Head â†’ Logits

1ï¸âƒ£ Modern SSM (ModernSSM)
	â€¢	Depthwise causal conv prefilter â†’ selective scan (A diag param via log stabilization).
	â€¢	Input-dependent gates (g_in, g_forget) control temporal propagation.
Role: capture long-range dependencies efficiently.

2ï¸âƒ£ Mixture-of-Recursions Hopfield (MoRHopfield)
	â€¢	Performs K recursive refinements (K scheduled per layer, e.g. [3, 2, 2, â€¦ 1]).(L0 Layer was ruled as most impactful after several ablations)
	â€¢	Learns to gate across recursion depths for shallow vs deep recall.
Role: robust pattern completion and associative memory.

3ï¸âƒ£ Causal Self-Attention (SelfAttention)
	â€¢	Standard multi-head attention with causal mask.
Role: content-based pattern mixing within the context window.

4ï¸âƒ£ Gated Mixer (GatedMixer)
	â€¢	Small feed-forward net generating softmax gates over SSM, Hopfield, and Attention outputs.
Role: learned information routing per context.

5ï¸âƒ£ Segmented Reasoner & Controller (SegmentReasoner, HController)
	â€¢	Two-segment forward loop; Segment 1 produces a hidden state fed to HController,
which outputs a bias vector to influence Segment 2â€™s mixing weights.
	â€¢	A stop_gradient approximates one-step credit assignment.
Role: iterative refinement without explicit RL.

â¸»

ğŸ“Š Performance and Experimental Results

 Main Results (WikiText-103 word-level)

Metric	Value
Best Validation Cross-Entropy	3.4424
Best Validation Perplexity	30.3
Training Steps	25 000
Parameters	â‰ˆ 107 M
Throughput (TPU v6e1)	â‰ˆ 60 k tok/s

A BPE-tokenized variant (~97 M params) reached PPL â‰ˆ 36.8.

â¸»

ğŸ“ˆ Training Dynamics

Early byte-level experiments confirmed stable learning curves with steady CE reduction, validating the architecture before scaling to word level.

(Figure 1 placeholder â€” Training and Validation CE curves)

â¸»

ğŸ”¬ Ablation Studies â€” Importance of Hybridization

Configuration	Validation CE	Category
Full Hybrid Model	3.60	Baseline
Only SSM	9.81	Single Path
Only Hopfield	7.72	Single Path
Only Attention	9.15	Single Path
SSM + Hopfield	5.43	Two Paths
SSM + Attention	9.51	Two Paths
Hopfield + Attention	7.40	Two Paths

Conclusion: All three components are necessary; removing any causes a sharp performance drop.

â¸»

ğŸ§© Probe Analysis â€” Component Roles

Intervention	Validation CE	Observation
Baseline (no shuffle)	3.59	â€”
Shuffle â†’ Attention	4.35	Moderate impact
Shuffle â†’ SSM	6.00	Strong impact
Shuffle â†’ Hopfield	8.01	Severe degradation

Findings
	â€¢	Hopfield modules encode position-dependent patterns.
	â€¢	Attention focuses on content irrespective of position.
	â€¢	SSM balances both, bridging sequential and content signals.

(Table 2 placeholder â€” Probe Results)

test	val_ce	category
baseline	3.604276418685913	baseline
seg_1	3.8901983737945556	segments
seg_2	3.613628625869751	segments
seg_3	3.604276418685913	segments
ctrl_normal	3.604276418685913	controller
ctrl_shuffle	3.7270468711853026	controller
ctrl_random	4.109416770935058	controller
no_ctrl	3.8901983737945556	controller
only_ssm	9.812429809570313	single_path
only_hop	7.717465019226074	single_path
only_attn	9.153304100036621	single_path
ssm+hop	5.428957176208496	two_path
ssm+attn	9.511291885375977	two_path
hop+attn	7.402231025695801	two_path

â¸»

Project Evolution

Phase	Focus	Milestone
1ï¸âƒ£ PyTorch Prototype	HippocampalMemoryInterface & Delayed Copy Task	Proof of concept
2ï¸âƒ£ Component Comparison	Recurrent vs Hopfield CA3	Hopfield proved superior
3ï¸âƒ£ Hybridization	Combine SSM + Hopfield + Attention on TinyShakespeare	Stable training
4ï¸âƒ£ JAX/Flax Migration	TPU performance + byte-level WikiText	Improved efficiency
5ï¸âƒ£ Scaling	Word-level WikiText-103 (~107 M params)	Final architecture + MoR/HRM-lite
6ï¸âƒ£ Advanced Concepts	Rectified Flow regularization	Stable long-runs and probes


â¸»

âš™ï¸ Usage

Setup

pip install jax flax optax datasets tokenizers

Use a TPU-enabled runtime for best performance.

Training

# From training_bpe.py
if __name__ == "__main__":
    train_bpe_level()

Sampling

import jax, pickle
from tokenizers import Tokenizer
import training_bpe as mdl

with open("path/to/best.ckpt", "rb") as f:
    ckpt = pickle.load(f)
tokenizer = Tokenizer.from_file(ckpt["tokenizer_path"])

prompt = "The history of artificial intelligence began in"
generated = mdl.sample_text(
    state=ckpt['ema_params'],
    base_cfg=ckpt['base_cfg'],
    tokenizer=tokenizer,
    prompt=prompt,
    max_new_tokens=100,
)
print(generated)


â¸»

ğŸ§± Code Structure

ProjectHippocampus/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hybrid_lm.py          # HybridLM wrapper
â”‚   â”œâ”€â”€ hybrid_block.py       # Core HybridBlock
â”‚   â”œâ”€â”€ modern_ssm.py         # SSM implementation
â”‚   â”œâ”€â”€ mor_hopfield.py       # Mixture-of-Recursions Hopfield
â”‚   â”œâ”€â”€ gated_mixer.py        # Learned gating
â”‚   â””â”€â”€ controller.py         # HRM-lite controller
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ training_bpe.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ rectified_flow.py
â”œâ”€â”€ probes/
â”‚   â”œâ”€â”€ shuffle_probes.py
â”‚   â””â”€â”€ results.csv
â”œâ”€â”€ checkpoints/
â””â”€â”€ README.md


â¸»

ğŸ“š Citation

@misc{adam2025hippocampus,
  title  = {Project Hippocampus: A Hybrid Language Model Architecture},
  author = {Julian Adam},
  year   = {2025},
  note   = {Hybrid SSM + Hopfield + Attention; HRM-lite controller; WikiText-103 word-level PPL â‰ˆ 31.3.}
}


â¸»

ğŸ“„ License

MIT License â€” see LICENSE.

â¸»
